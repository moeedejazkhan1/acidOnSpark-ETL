# ğŸš€ AcidOnSpark-ETL

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)
![Docker](https://img.shields.io/badge/Docker-Ready-blue?logo=docker)
![Spark](https://img.shields.io/badge/Apache%20Spark-ETL-orange?logo=apachespark)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-Orchestration-brightgreen?logo=apacheairflow)
![License](https://img.shields.io/badge/License-MIT-green)

A **modern ETL pipeline** that combines the power of **Spark, Airflow, MinIO, Delta Lake, Hive, Presto, and Superset** into a fully containerized data ecosystem.  
Built for **scalable, fault-tolerant, and analytics-ready workflows**.  

ğŸ“– Learn more here:  
ğŸ‘‰ [Doing ACID on Spark (Medium)]

---

## âš¡ï¸ Features

- End-to-End ETL pipeline â€“ from ingestion to visualization  
- Fully containerized using Docker  
- Horizontally scalable with Spark & Presto  
- ACID-compliant storage with Delta Lake  
- SQL-ready via Hive Metastore + Presto  
- Interactive dashboards with Superset  

---

## ğŸ› ï¸ Tech Stack

ğŸ Python 3 Â· ğŸ³ Docker Â· ğŸ”¥ Apache Spark Â· ğŸŒ¬ Airflow Â· ğŸ“¦ MinIO  
ğŸ—‚ Delta Lake Â· ğŸ Hive Â· ğŸ¬ MariaDB Â· âš¡ Presto Â· ğŸ“Š Superset  

---

## ğŸ”§ How It Works

All components are containerized and can be started with simple `make` commands:

```bash
# ğŸ”¥ Spark
make spark        # Launch Spark master + one worker
make scale-spark  # Scale Spark workers horizontally

# ğŸŒ¬ Airflow
make airflow      # Launch Airflow for workflow orchestration

# ğŸ“¦ MinIO
make minio        # Start S3-compatible storage for data lake & Hive tables

# ğŸ—‚ Hive + ğŸ¬ MariaDB
make hive         # Launch Hive Metastore + MariaDB as metastore DB

# âš¡ Presto
make presto-cluster  # Launch Presto coordinator + worker(s)
make presto-cli      # Open interactive Presto SQL CLI

# ğŸ“Š Superset
make superset     # Launch Superset for dashboards & BI
